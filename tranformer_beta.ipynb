{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XLq1tM_SsgPF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnAX-3tVsgPH",
        "outputId": "4ad02697-988d-4152-9eca-7e4299be2855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available() # For macOS\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KreR0leEsgPI"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Patch the image (needs to be square) and performs a linear projection of the patchs see : \"\"\"\n",
        "    def __init__(self, img_size, patch_size, in_channels=3, embedding_dim=512):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.n_patches = (self.img_size // patch_size) ** 2\n",
        "        self.proj_layer = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x : [n_batches, in_channels, img_size, img_size]\n",
        "            output : [n_batches, embedding_dim, n_batches]\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.proj_layer(x) #[n_bathces, embedding_dim, sqrt(n_patches), sqrt(n_pathces)]\n",
        "        x = x.flatten(2) #[n_batches, embedding_dim, n_patches]\n",
        "\n",
        "        return x\n",
        "\n",
        "class EncoderDecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, n_heads, mlp_ratio=4, p_dropout=0.5):\n",
        "        super(EncoderDecoderBlock, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "        self.p_dropout = p_dropout\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.norm1 = nn.LayerNorm(self.dim)\n",
        "        self.norm2 = nn.LayerNorm(self.dim)\n",
        "        self.norm3 = nn.LayerNorm(self.dim)\n",
        "        self.cross_attention = nn.MultiheadAttention(self.dim, self.n_heads, dropout=self.p_dropout, batch_first=True).to(device)\n",
        "        self.first_attention = nn.MultiheadAttention(self.dim, self.n_heads, dropout=self.p_dropout, batch_first=True).to(device)\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(self.dim, self.dim * mlp_ratio),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(self.p_dropout),\n",
        "            nn.Linear(self.dim * mlp_ratio, self.dim),\n",
        "            nn.Dropout(self.p_dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, features):\n",
        "        \"\"\"\n",
        "        x : [n_samples, n_patches + 1, embedding_dim]\n",
        "        output : [n_samples, n_patches + 1, embedding_dim]\n",
        "        \"\"\"\n",
        "        attention_out, attn1_weights = self.first_attention(x, x, x)\n",
        "        first_out = self.norm1(attention_out + x)\n",
        "        cross_attention, attn2_weights = self.cross_attention(first_out.to(device), features.to(device), features.to(device))\n",
        "        second_out = self.norm2(first_out + cross_attention)\n",
        "        mlp_out = self.MLP(second_out)\n",
        "        output = self.norm3(mlp_out + second_out)\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kMBLU52X150A"
      },
      "outputs": [],
      "source": [
        "# Load captions from the text file\n",
        "with open(os.path.join('./', 'captions.txt'), 'r') as f:\n",
        "    next(f)\n",
        "    captions_doc = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c5ea86ee3841448d955cf590cf7871cb",
            "469e5633e73142478f8e7eaf6a75e0b9",
            "7b28d168b27247ee8680067076585aec",
            "211606d0f2274a31b9f5ae8561a4c2d1",
            "302d1332b8494122af6ef3bf2bdcb942",
            "021e624c01184da1845e7d55c1407084",
            "f62d415399dc496d8ee9507b5f8a69dd",
            "864714ce31f44366bfbfc4986e0908e2",
            "ac4a7f22e74c40748c28edfea5e13527",
            "4a7bf364722445d19460ee12cb53d475",
            "23633c31a71f491bb29d050aa69e88b4"
          ]
        },
        "id": "m6BDJpvA13ON",
        "outputId": "1f32ef1b-eeab-4606-e90f-527d796ab110"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "573cf443637e453c9cd26049f0449d2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40456 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create mapping of image to captions\n",
        "mapping = {}\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    tokens = line.split(',')\n",
        "    if len(line) < 2:\n",
        "        continue\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    image_id = image_id.split('.')[0]\n",
        "    caption = \" \".join(caption)\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    mapping[image_id].append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j7YZUYsb18ys"
      },
      "outputs": [],
      "source": [
        "# Clean the captions\n",
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            caption = captions[i]\n",
        "            caption = caption.lower()\n",
        "            caption = caption.replace('[^A-Za-z]', '')\n",
        "            caption = caption.replace('\\s+', ' ')\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
        "            captions[i] = caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uqVvS9eV1_jt"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text\n",
        "clean(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwI0HLcq2Caq",
        "outputId": "c4e0085f-aab2-475a-df0a-8472756b2c16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40455"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_captions = [caption for captions in mapping.values() for caption in captions]\n",
        "len(all_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdy3Fhl41v1k",
        "outputId": "8aac6c8f-5da5-43c4-ad7f-6ab60ed72bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8896\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [tokenizer(caption) for caption in all_captions]\n",
        "\n",
        "# Build vocabulary : Mapping every token to an integer index\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_text)\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu9mEH2F1yPG",
        "outputId": "a9d56704-108e-4452-cf16-09f729671db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35\n"
          ]
        }
      ],
      "source": [
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "print(max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EnaHgWZb2eGx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def one_hot(a, num_classes):\n",
        "\n",
        "    out = np.zeros(num_classes)\n",
        "    out[a] = 1\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "D0G-32gdcPA8"
      },
      "outputs": [],
      "source": [
        "class CaptioningDataset(Dataset):\n",
        "  def __init__(self, data_keys, mapping, transform, tokenizer, max_length):\n",
        "    self.data_keys = data_keys\n",
        "    self.mapping = mapping\n",
        "    self.transform = transform\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_keys)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      key = self.data_keys[idx]\n",
        "      captions = self.mapping[key]\n",
        "\n",
        "      caption = captions[np.random.choice(len(captions))]\n",
        "      input2, y = torch.zeros(self.max_length).int(), torch.zeros((self.max_length, vocab_size))\n",
        "\n",
        "      tokens = self.tokenizer(caption)\n",
        "      caption_indices = [vocab[token] for token in tokens]\n",
        "\n",
        "      image = Image.open('Images/' + self.data_keys[idx] + '.jpg')\n",
        "      image = self.transform(image)\n",
        "      # print(\"1\", input2.shape)\n",
        "      for i in range(1, len(caption_indices)):\n",
        "          in_seq, out_seq = caption_indices[i-1], caption_indices[i]\n",
        "\n",
        "          out_seq = int(out_seq)\n",
        "\n",
        "          #in_seq = in_seq[:self.max_length] + [0] * max(0, self.max_length - len(in_seq))\n",
        "          out_seq = one_hot(out_seq, num_classes=vocab_size)\n",
        "          input2[i-1] = int(in_seq)\n",
        "          # print(\"2\", input2.shape)\n",
        "\n",
        "          y[i-1] = torch.as_tensor(out_seq)\n",
        "      # print(\"3\", input2.shape)\n",
        "      # y = pad_sequence([y, dummy3])[:, 0, :]\n",
        "      # input1 = pad_sequence([torch.tensor(input1), dummy1])[:, 0, :]\n",
        "      # input2 = pad_sequence([torch.tensor(input2), dummy2])[:, 0, :]\n",
        "      # print(\"4\", input2.shape)\n",
        "      # print(input2)\n",
        "      return image.transpose(0,2), input2, y\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l0MO0wEh21Y0"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gnfzPEGg4OMA"
      },
      "outputs": [],
      "source": [
        "image_ids = list(mapping.keys())\n",
        "split = int(len(image_ids) * 0.75)\n",
        "train = image_ids[:split]\n",
        "test = image_ids[split:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hjYvabxM4zOD"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_dataset = CaptioningDataset(train, mapping, transform, tokenizer, max_length)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aQHuJCjw4_rL"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_dataset = CaptioningDataset(test, mapping, transform, tokenizer, max_length)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWY7v_LO5FSw",
        "outputId": "bb5b7cae-a4f7-43b0-8367-b8a94a061068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([224, 224, 3])\n",
            "torch.Size([35])\n",
            "torch.Size([35, 8896])\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.__getitem__(0)[0].size())\n",
        "print(train_dataset.__getitem__(0)[1].size())\n",
        "print(train_dataset.__getitem__(0)[2].size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "A7ibHm2C5yQ-"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "  def __init__(self, vit, encoder_decoder, vocab_size, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.vit = vit\n",
        "    self.transformer = encoder_decoder.to(device)\n",
        "\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "    self.decoder = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, image, input2):\n",
        "    vit_out = self.vit(image.cpu().numpy()).numpy()\n",
        "    vit_out = torch.from_numpy(vit_out)\n",
        "    embedding_out = self.embedding(input2)\n",
        "\n",
        "    output = self.transformer(embedding_out, vit_out)\n",
        "\n",
        "    output = self.decoder(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaLmw8Xd-d56",
        "outputId": "53f844b3-b983-42f9-885f-4c3644f4e356"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-14 16:23:21.056067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-14 16:23:21.937358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/home/aabibi/IA/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
        "\n",
        "\n",
        "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ph8vS2-WAiWI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    image, inputs2, targets = batch\n",
        "    features = feature_extractor(image)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3zGAkkcAyu9",
        "outputId": "897b8e61-98ed-4a7b-b0c5-9b18ae0d251d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/aabibi/IA/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "2023-12-14 16:23:24.517062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 16:23:24.606449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 16:23:24.606617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 16:23:24.607926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 16:23:24.608034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 16:23:24.608123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 16:23:24.672909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 16:23:24.673077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 16:23:24.673177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 16:23:24.673261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10392 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
            "/home/aabibi/IA/lib/python3.11/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 12, 12 to 7, 7\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from vit_keras import vit\n",
        "vit_model = vit.vit_b32(\n",
        "        image_size = (224,224),\n",
        "        activation = 'softmax',\n",
        "        pretrained = True,\n",
        "        include_top = False,\n",
        "        pretrained_top = False,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DGzKpBeDfwG",
        "outputId": "2141a327-e367-43f6-ac64-fd69c0ee0bbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-14 16:23:29.331248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 2s 9ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-14 16:23:29.960777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 0.64806175,  0.74852777, -0.3822827 , ...,  0.06661464,\n",
              "         0.7493057 , -1.1575644 ],\n",
              "       [ 0.26736924,  1.7556778 ,  0.77989626, ..., -0.19642167,\n",
              "         0.5085754 , -0.9234603 ],\n",
              "       [ 0.21296726, -0.21295676, -1.0947556 , ...,  0.46254605,\n",
              "        -0.86662513, -0.4105462 ],\n",
              "       ...,\n",
              "       [ 0.4376147 ,  1.9498951 , -2.073971  , ...,  0.4937921 ,\n",
              "         0.30152392,  0.0212844 ],\n",
              "       [ 0.8694054 ,  1.1882443 ,  0.5216992 , ...,  0.5407412 ,\n",
              "        -0.44117576,  0.03475773],\n",
              "       [ 0.18222791,  0.91645265, -2.2571015 , ...,  0.26988328,\n",
              "        -1.2860185 , -0.42763546]], dtype=float32)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "feat = vit_model.predict(image.numpy())\n",
        "feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g2pyELAEplt",
        "outputId": "a12eebdc-21b4-4a72-b574-eafa6b654686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(64, 768)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nQ4HHOz9NRnO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "new_input = vit_model.input\n",
        "hidden_layer = vit_model.layers[-2].output\n",
        "vision_transformer_model = tf.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHKdZSc3NapD",
        "outputId": "5cfb17ca-3f81-4351-87d3-6de9df40a91f"
      },
      "outputs": [],
      "source": [
        "featt = vision_transformer_model(image.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYkIj0SkNUSv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tWEfQPNaL0Vd"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "embedding_size = 768\n",
        "hidden_size = 256\n",
        "n_heads = 12\n",
        "depth = 1\n",
        "dropout = 0.5\n",
        "\n",
        "transformer = EncoderDecoderBlock(embedding_size,n_heads)\n",
        "\n",
        "\n",
        "model = ImageCaptioningModel(vision_transformer_model, transformer , vocab_size, embedding_size)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-grJwFiIL19u"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TWybVMlaUNxc"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = torch.logical_not(torch.eq(real, torch.zeros_like(real)))\n",
        "    print(mask.size())\n",
        "    loss_ = criterion(real[mask], pred[mask])\n",
        "    mask = mask.to(dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return torch.sum(loss_) / torch.sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qvd-8AcZWdA"
      },
      "outputs": [],
      "source": [
        "targets.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzF8KVxrYX1M",
        "outputId": "68970a2c-c55e-4a14-b5f3-7351cdb5cfe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([637])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "targets[torch.logical_not(torch.eq(targets, torch.zeros_like(targets)))].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "XE1UF4p8LNqS",
        "outputId": "2d73ed82-dd0f-455f-98d6-1e7afd4ab644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Loss: 4405.0879\n",
            "Epoch [2/50], Loss: 4279.7670\n",
            "Epoch [3/50], Loss: 4271.7992\n",
            "Epoch [4/50], Loss: 4320.0488\n",
            "Epoch [5/50], Loss: 4262.4734\n",
            "Epoch [6/50], Loss: 4263.0032\n",
            "Epoch [7/50], Loss: 4272.9275\n",
            "Epoch [8/50], Loss: 4282.2784\n",
            "Epoch [9/50], Loss: 4303.9345\n",
            "Epoch [10/50], Loss: 4285.5711\n",
            "Epoch [11/50], Loss: 4286.8969\n",
            "Epoch [12/50], Loss: 4295.9728\n",
            "Epoch [13/50], Loss: 4287.2005\n",
            "Epoch [14/50], Loss: 4267.2739\n",
            "Epoch [15/50], Loss: 4276.3354\n",
            "Epoch [16/50], Loss: 4283.2896\n",
            "Epoch [17/50], Loss: 4287.9636\n",
            "Epoch [18/50], Loss: 4259.7562\n",
            "Epoch [19/50], Loss: 4285.7737\n",
            "Epoch [20/50], Loss: 4312.4996\n",
            "Epoch [21/50], Loss: 4281.1047\n",
            "Epoch [22/50], Loss: 4267.7041\n",
            "Epoch [23/50], Loss: 4269.2603\n",
            "Epoch [24/50], Loss: 4245.9423\n",
            "Epoch [25/50], Loss: 4255.5041\n",
            "Epoch [26/50], Loss: 4265.8577\n",
            "Epoch [27/50], Loss: 4268.9237\n",
            "Epoch [28/50], Loss: 4267.8221\n",
            "Epoch [29/50], Loss: 4256.5229\n",
            "Epoch [30/50], Loss: 4267.9430\n",
            "Epoch [31/50], Loss: 4277.2751\n",
            "Epoch [32/50], Loss: 4295.8732\n",
            "Epoch [33/50], Loss: 4293.0590\n",
            "Epoch [34/50], Loss: 4291.8394\n",
            "Epoch [35/50], Loss: 4252.7877\n",
            "Epoch [36/50], Loss: 4285.9267\n",
            "Epoch [37/50], Loss: 4253.1466\n",
            "Epoch [38/50], Loss: 4260.8741\n",
            "Epoch [39/50], Loss: 4258.4755\n",
            "Epoch [40/50], Loss: 4263.4959\n",
            "Epoch [41/50], Loss: 4285.5276\n",
            "Epoch [42/50], Loss: 4240.4852\n",
            "Epoch [43/50], Loss: 4267.7708\n",
            "Epoch [44/50], Loss: 4286.7909\n",
            "Epoch [45/50], Loss: 4265.4406\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        image, inputs2, targets = batch\n",
        "\n",
        "        image, inputs2, targets = image.to(device), inputs2.to(device), targets.to(device)\n",
        "        # Generate output sequence from the model\n",
        "        output = model(image, inputs2)\n",
        "\n",
        "        mask = torch.logical_not(torch.eq(targets, torch.zeros_like(targets)))\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output[mask], targets[mask])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcsOi1l7V4sT"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "def idx_to_word(index):\n",
        "    try:\n",
        "        return vocab.get_itos()[index]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def predict_caption(model, image_path, max_length):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path)\n",
        "    image = transform(image).transpose(0,2)\n",
        "    input2 = torch.zeros(1).int()\n",
        "    in_text = 'startseq'\n",
        "    for _ in range(max_length):\n",
        "        input2[0] = torch.as_tensor(vocab[in_text.split(' ')[-1]], dtype=torch.int64)\n",
        "        input2 = input2.to(device)\n",
        "\n",
        "        outputs = model(image, input2)\n",
        "\n",
        "        outputs = F.softmax(outputs, dim=1)\n",
        "\n",
        "        y_pred = torch.argmax(outputs, dim=1).squeeze(0).item()\n",
        "\n",
        "        word = idx_to_word(y_pred)\n",
        "        in_text += ' ' + word\n",
        "\n",
        "        if word is None or word == 'endseq' :\n",
        "            break\n",
        "\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9AzlL1sKgTj"
      },
      "outputs": [],
      "source": [
        "predict_caption(model, \"Image/3736786640_70df13be2c\", 35)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "021e624c01184da1845e7d55c1407084": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "211606d0f2274a31b9f5ae8561a4c2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a7bf364722445d19460ee12cb53d475",
            "placeholder": "​",
            "style": "IPY_MODEL_23633c31a71f491bb29d050aa69e88b4",
            "value": " 40456/40456 [00:00&lt;00:00, 181803.79it/s]"
          }
        },
        "23633c31a71f491bb29d050aa69e88b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "302d1332b8494122af6ef3bf2bdcb942": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "469e5633e73142478f8e7eaf6a75e0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_021e624c01184da1845e7d55c1407084",
            "placeholder": "​",
            "style": "IPY_MODEL_f62d415399dc496d8ee9507b5f8a69dd",
            "value": "100%"
          }
        },
        "4a7bf364722445d19460ee12cb53d475": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b28d168b27247ee8680067076585aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_864714ce31f44366bfbfc4986e0908e2",
            "max": 40456,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac4a7f22e74c40748c28edfea5e13527",
            "value": 40456
          }
        },
        "864714ce31f44366bfbfc4986e0908e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac4a7f22e74c40748c28edfea5e13527": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5ea86ee3841448d955cf590cf7871cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_469e5633e73142478f8e7eaf6a75e0b9",
              "IPY_MODEL_7b28d168b27247ee8680067076585aec",
              "IPY_MODEL_211606d0f2274a31b9f5ae8561a4c2d1"
            ],
            "layout": "IPY_MODEL_302d1332b8494122af6ef3bf2bdcb942"
          }
        },
        "f62d415399dc496d8ee9507b5f8a69dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
