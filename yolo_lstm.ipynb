{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "yolo_model = yolo_model.model.model[:10]\n",
    "yolo_model = yolo_model.to(device)\n",
    "yolo_model.eval()\n",
    "print(yolo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from images\n",
    "features = {}\n",
    "directory = 'Images'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    img_path = os.path.join(directory, img_name)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    feature = yolo_model(image)\n",
    "    feature = feature.view(feature.size(0), -1).detach().cpu().numpy()[0]\n",
    "    image_id = img_name.split('.')[0]\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store features in pickle\n",
    "with open(os.path.join('./', 'features.pkl'), 'wb') as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from pickle\n",
    "with open(os.path.join('./', 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features['3250076419_eb3de15063'])\n",
    "print(features['3747543364_bf5b548527'])\n",
    "\n",
    "print(features['3250076419_eb3de15063'].shape)\n",
    "print(len(features))\n",
    "feature_size = features['3250076419_eb3de15063'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions from the text file\n",
    "with open(os.path.join('./', 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping of image to captions\n",
    "mapping = {}\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    image_id = image_id.split('.')[0]\n",
    "    caption = \" \".join(caption)\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the captions\n",
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i]\n",
    "            caption = caption.lower()\n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before preprocess of text\n",
    "print(mapping['33108590_d685bfe51c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "clean(mapping)\n",
    "\n",
    "# After preprocess of text\n",
    "print(mapping['33108590_d685bfe51c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = [caption for captions in mapping.values() for caption in captions]\n",
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = [tokenizer(caption) for caption in all_captions]\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_text)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab['you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids)*0.75)\n",
    "train = image_ids[:256]\n",
    "# train_captions = all_captions[:32*5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = image_ids[7900:]\n",
    "# test_captions = all_captions[7900*5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a, num_classes):\n",
    "    out = np.zeros(num_classes)\n",
    "    out[a] = 1\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, data_keys, mapping, features, tokenizer, max_length):\n",
    "        self.data_keys = data_keys\n",
    "        self.mapping = mapping\n",
    "        self.features = features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.data_keys[idx]\n",
    "        captions = self.mapping[key]\n",
    "\n",
    "        # caption = captions[np.random.choice(len(captions))]\n",
    "        caption = captions[0]\n",
    "        input1, input2, y = torch.zeros((1, feature_size)), torch.zeros(max_length).int(), torch.zeros((max_length, vocab_size))\n",
    "        input1[0] = torch.as_tensor(features[key])\n",
    "        tokens = self.tokenizer(caption)\n",
    "        caption_indices = [vocab[token] for token in tokens]\n",
    "\n",
    "        for i in range(1, len(caption_indices)):\n",
    "            in_seq, out_seq = caption_indices[i-1], caption_indices[i]\n",
    "\n",
    "            out_seq = int(out_seq)\n",
    "            out_seq = one_hot(out_seq, num_classes=vocab_size)\n",
    "         \n",
    "            input2[i-1] = int(in_seq)\n",
    "\n",
    "\n",
    "            y[i-1] = torch.as_tensor(out_seq)\n",
    "\n",
    "        return input1, input2, y, idx\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     caption = self.data_keys[idx]\n",
    "    #     key = self.mapping[idx//5]\n",
    "        \n",
    "    #     input1, input2, y = torch.zeros((1, feature_size)), torch.zeros(max_length).int(), torch.zeros((max_length, vocab_size))\n",
    "    #     input1[0] = torch.as_tensor(features[key])\n",
    "    #     tokens = self.tokenizer(caption)\n",
    "    #     caption_indices = [vocab[token] for token in tokens]\n",
    "\n",
    "    #     for i in range(1, len(caption_indices)):\n",
    "    #         in_seq, out_seq = caption_indices[i-1], caption_indices[i]\n",
    "\n",
    "    #         out_seq = int(out_seq)\n",
    "    #         out_seq = one_hot(out_seq, num_classes=vocab_size)\n",
    "         \n",
    "    #         input2[i-1] = int(in_seq)\n",
    "\n",
    "\n",
    "    #         y[i-1] = torch.as_tensor(out_seq)\n",
    "\n",
    "    #     return input1, input2, y, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# train_dataset = CaptionDataset(train_captions, train, features, tokenizer, max_length)\n",
    "train_dataset = CaptionDataset(train, mapping, features, tokenizer, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.__getitem__(0)[0].size())\n",
    "print(train_dataset.__getitem__(0)[1].size())\n",
    "print(train_dataset.__getitem__(0)[2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, feature_size, hidden_size) :\n",
    "        super(Encoder, self).__init__()\n",
    "        self.image_feature_layer = nn.Sequential(\n",
    "            # nn.Dropout(0.4),\n",
    "            nn.Linear(feature_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, hidden_size),   \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, image_input) :\n",
    "        return self.image_feature_layer(image_input) \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.lstm_cell = nn.LSTMCell(embedding_size, hidden_size)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, fe, sequence_input, h, c):\n",
    "        se = self.emb(sequence_input)\n",
    "        outputs = []\n",
    "        if len(se.size()) == 3 :\n",
    "            for t in range(se.size(1)) :\n",
    "                h, c = self.lstm_cell(se[:, t, :], (h+fe, c))\n",
    "                output_t = self.decoder(h)\n",
    "                outputs.append(output_t)\n",
    "            outputs = torch.stack(outputs, dim=1)\n",
    "   \n",
    "        else : \n",
    "            h, c = self.lstm_cell(se.squeeze(0), (h+fe, c))\n",
    "            outputs = self.decoder(h)\n",
    "            \n",
    "\n",
    "        return outputs, h, c    \n",
    "\n",
    "class ImageCaptioningModel(nn.Module) :\n",
    "    def __init__(self, feature_size, vocab_size, embedding_size, hidden_size):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = Encoder(feature_size, hidden_size)\n",
    "        self.decoder = Decoder(vocab_size, embedding_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, image_input, sequence_input, h=None, c=None) :\n",
    "        fe = self.encoder(image_input)\n",
    "        c = None\n",
    "        if h is None :\n",
    "            if len(fe.size()) == 3 :\n",
    "                fe = fe.permute(1, 0, 2)\n",
    "                fe = fe.squeeze(0)\n",
    "                c = torch.randn(fe.shape[0], self.hidden_size).to(device)\n",
    "            \n",
    "            else : \n",
    "                fe = fe.squeeze(0)\n",
    "                c = torch.randn(self.hidden_size).to(device) \n",
    "            return self.decoder(fe, sequence_input, fe, c)\n",
    "        \n",
    "        c = torch.randn(self.hidden_size).to(device) \n",
    "        return self.decoder(fe, sequence_input, h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "embedding_size = 256\n",
    "hidden_size = 256\n",
    "\n",
    "model = ImageCaptioningModel(feature_size, vocab_size, embedding_size, hidden_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def idx_to_word(index):\n",
    "    try:\n",
    "        return vocab.get_itos()[index]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def predict_caption(model, feature, max_length) :\n",
    "    model.eval()\n",
    "    input1 = torch.zeros((1, feature_size)).to(device)\n",
    "    input1[0] = torch.as_tensor(feature)\n",
    "    input1 = input1.squeeze(0)\n",
    "    hidden = None\n",
    "    c = None \n",
    "    input2 = torch.zeros(1).int()\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length) :\n",
    "        input2[0] = torch.as_tensor(vocab[in_text.split(' ')[-1]], dtype=torch.int64)\n",
    "        input2 = input2.to(device)\n",
    "        outputs, hidden, c = model(input1, input2, hidden, c)\n",
    "\n",
    "        outputs = F.softmax(outputs, dim=0)\n",
    "\n",
    "        # y_pred = torch.argmax(outputs, dim=1).squeeze(0).item()\n",
    "        y_pred = torch.multinomial(outputs, 1).squeeze(0).item()\n",
    "        \n",
    "        word = idx_to_word(y_pred)\n",
    "        in_text += ' ' + word\n",
    "        \n",
    "        if word is None or word == 'endseq' :\n",
    "            break\n",
    "\n",
    "    return in_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 150\n",
    "                                                         \n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_train = 0\n",
    "    total_loss_test = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs1, inputs2, targets, _ = batch\n",
    "\n",
    "        inputs1, inputs2, targets = inputs1.to(device), inputs2.to(device), targets.to(device)\n",
    "        hidden, c = None, None\n",
    "        output, hidden, c = model(inputs1, inputs2, hidden, c)\n",
    "\n",
    "        mask = torch.sum(targets, dim=-1) != 0\n",
    "        output_flat = output.view(-1, vocab_size)\n",
    "        targets_flat = targets.view(-1, vocab_size)\n",
    "\n",
    "        output_masked = output_flat[mask.view(-1)]\n",
    "        targets_masked = targets_flat[mask.view(-1)]\n",
    "        loss = criterion(output_masked, targets_masked)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "    average_loss_train = total_loss_train / len(train_loader)\n",
    "    scheduler.step(average_loss_train)\n",
    "\n",
    "    # if epoch % 30 == 0 or epoch == num_epochs - 1 :\n",
    "    #     actual, predicted = [], []\n",
    "    #     for key in train :\n",
    "    #         captions = mapping[key]\n",
    "    #         y_pred = predict_caption(model, features[key], max_length)\n",
    "            \n",
    "    #         actual_captions = [caption.split() for caption in captions]\n",
    "    #         y_pred = y_pred.split()\n",
    "    #         actual.append(actual_captions)\n",
    "    #         predicted.append(y_pred)\n",
    "\n",
    "    #     bleu1_train = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
    "\n",
    "        # print(f'Epoch [{epoch + 1}/{num_epochs}], Loss Train: {average_loss_train:.4f}, BLEU-1 Score Test: {bleu1_train:.4f}')\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss Train: {average_loss_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actual, predicted = [], []\n",
    "\n",
    "for key in tqdm(test):\n",
    "    captions = mapping[key]\n",
    "    y_pred = predict_caption(model, features[key], max_length)\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
    "bleu2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
    "\n",
    "print(f\"BLEU-1: {bleu1}\")\n",
    "print(f\"BLEU-2: {bleu2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption for an image\n",
    "def generate_caption(image_name) :\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join('./', \"Images\", image_name)\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    captions = mapping[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "\n",
    "    y_pred = predict_caption(model, features[image_id], max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_caption(f'{train[1]}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_caption(f'{train[2]}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_caption(f'{train[3]}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_caption(f'{train[4]}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_caption(f'{test[25]}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_caption(f'{test[56]}.jpg'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
