{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available() # For macOS\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 Model\n",
    "model = models.vgg16(weights='DEFAULT')\n",
    " # Remove the last fully connected layer\n",
    "model = nn.Sequential(*list(model.features.children()))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Summarize\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from images\n",
    "features = {}\n",
    "directory = 'Images'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    img_path = os.path.join(directory, img_name)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    feature = model(image)\n",
    "    feature = feature.view(feature.size(0), -1).detach().cpu().numpy()[0]\n",
    "    image_id = img_name.split('.')[0]\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store features in pickle\n",
    "with open(os.path.join('./', 'features.pkl'), 'wb') as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from pickle\n",
    "with open(os.path.join('./', 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features['3250076419_eb3de15063'])\n",
    "print(features['3250076419_eb3de15063'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions from the text file\n",
    "with open(os.path.join('./', 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping of image to captions\n",
    "mapping = {}\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    image_id = image_id.split('.')[0]\n",
    "    caption = \" \".join(caption)\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the captions\n",
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i]\n",
    "            caption = caption.lower()\n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before preprocess of text\n",
    "print(mapping['1000268201_693b08cb0e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "clean(mapping)\n",
    "\n",
    "# After preprocess of text\n",
    "print(mapping['1000268201_693b08cb0e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = [caption for captions in mapping.values() for caption in captions]\n",
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = [tokenizer(caption) for caption in all_captions]\n",
    "\n",
    "# Build vocabulary : Mapping every token to an integer index\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_text)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab['you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.75)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data generator\n",
    "# class CaptionDataset(Dataset):\n",
    "#     def __init__(self, data_keys, mapping, features, tokenizer, max_length):\n",
    "#         self.data_keys = data_keys\n",
    "#         self.mapping = mapping\n",
    "#         self.features = features\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data_keys)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         key = self.data_keys[idx]\n",
    "#         captions = self.mapping[key]\n",
    "\n",
    "#         caption = captions[np.random.choice(len(captions))]\n",
    "\n",
    "#         # for caption in captions :\n",
    "#         tokens = self.tokenizer(caption)\n",
    "#         caption_indices = [vocab[token] for token in tokens]\n",
    "#         caption_indices = caption_indices[:self.max_length] + [0] * max(0, self.max_length - len(caption_indices))\n",
    "#         image_features = torch.tensor(self.features[key], dtype=torch.float32)\n",
    "#         caption_indices = torch.tensor(caption_indices, dtype=torch.long)\n",
    "\n",
    "#         return image_features, caption_indices\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_keys, mapping, features, tokenizer, max_length, vocab_size, vocab):\n",
    "        self.data_keys = data_keys\n",
    "        self.mapping = mapping\n",
    "        self.features = features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.data_keys[idx]\n",
    "        captions = self.mapping[key]\n",
    "        X1, X2, y = list(), list(), list()\n",
    "\n",
    "        for caption in captions:\n",
    "            seq = self.tokenizer(caption)\n",
    "            seq = [self.vocab[word] for word in seq]\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = in_seq[:self.max_length] + [0] * max(0, self.max_length - len(in_seq))\n",
    "                out_seq = torch.nn.functional.one_hot(torch.LongTensor([out_seq]), num_classes=self.vocab_size).squeeze(0)\n",
    "\n",
    "                X1.append(torch.Tensor(self.features[key]))\n",
    "                X2.append(torch.LongTensor(in_seq))\n",
    "                y.append(out_seq)\n",
    "\n",
    "        # Pad sequences to the maximum length in the batch\n",
    "        X2_padded = pad_sequence(X2, batch_first=True, padding_value=0)\n",
    "        y_padded = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "        # Convert out_seq to one-hot encoding\n",
    "        y_one_hot = one_hot(y_padded.view(-1), num_classes=self.vocab_size).view(y_padded.size(0), y_padded.size(1), -1)\n",
    "\n",
    "        return torch.stack(X1), X2_padded, y_one_hot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# train_dataset = CaptionDataset(train, mapping, features, tokenizer, max_length)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "train_loader = DataLoader(CustomDataset(train, mapping, features, tokenizer, max_length, vocab_size, vocab), batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        \n",
    "        # Image feature layers\n",
    "        self.image_feature_layer = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(25088, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Sequence feature layers\n",
    "        self.sequence_feature_layer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_size, padding_idx=0),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LSTM(embedding_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, vocab_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_input, sequence_input):\n",
    "        fe = self.image_feature_layer(image_input)\n",
    "        se, _ = self.sequence_feature_layer(sequence_input)\n",
    "        se = se[:, -1, :]\n",
    "        combined = torch.cat((fe, se), dim=1)\n",
    "        output = self.decoder(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "embedding_size = 256\n",
    "hidden_size = 256\n",
    "\n",
    "model = ImageCaptioningModel(vocab_size, embedding_size, hidden_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs1, inputs2, targets = batch\n",
    "        inputs1, inputs2, targets = inputs1.to(device), inputs2.to(device), targets.to(device)\n",
    "\n",
    "        # Generate output sequence from the model\n",
    "        output = model(inputs1, inputs2)\n",
    "\n",
    "        # Reshape the output and targets to have the same batch size\n",
    "        output = output.view(-1, vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        # Calculate the loss\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join('./', 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def idx_to_word(index, tokenizer):\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx == index:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    model.eval()\n",
    "\n",
    "    in_text = 'startseq'\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        inputs = [torch.tensor(image).to(device), tokenizer.encode_plus(in_text, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt').to(device)]\n",
    "        outputs = model(*inputs)\n",
    "        y_pred = torch.argmax(outputs[1], dim=2).squeeze(0)[-1].item()\n",
    "\n",
    "        word = idx_to_word(y_pred, tokenizer)\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "\n",
    "        in_text += ' ' + word\n",
    "\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BLEU Score Calculation\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "actual, predicted = [], []\n",
    "\n",
    "for key in tqdm(test):\n",
    "    captions = mapping[key]\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "bleu1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
    "bleu2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
    "\n",
    "print(f\"BLEU-1: {bleu1:.4f}\")\n",
    "print(f\"BLEU-2: {bleu2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption for an image\n",
    "def generate_caption(image_name):\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join('./', \"Images\", image_name)\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    captions = mapping[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "\n",
    "    plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"1001773457_577c3a7d70.jpg\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
