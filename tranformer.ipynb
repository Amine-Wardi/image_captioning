{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Patch the image (needs to be square) and performs a linear projection of the patchs see : \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels=3, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.n_patches = (self.img_size // patch_size) ** 2\n",
    "        self.proj_layer = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x : [n_batches, in_channels, img_size, img_size]\n",
    "            output : [n_batches, embedding_dim, n_batches]\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.proj_layer(x) #[n_bathces, embedding_dim, sqrt(n_patches), sqrt(n_pathces)]\n",
    "        x = x.flatten(2) #[n_batches, embedding_dim, n_patches]\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4, p_dropout=0.5):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.p_dropout = p_dropout\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.norm = nn.LayerNorm(self.latent_size)\n",
    "        self.attention = nn.MultiheadAttention(self.dim, self.n_heads, dropout=self.p_dropout)\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.dim * mlp_ratio, self.dim),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : [n_samples, n_patches + 1, embedding_dim]\n",
    "        output : [n_samples, n_patches + 1, embedding_dim]\n",
    "        \"\"\"\n",
    "        first_norm = self.norm(x)\n",
    "        attention_out = self.attention(first_norm, first_norm, first_norm)\n",
    "        first_added = attention_out + x\n",
    "        second_norm = self.norm(first_added)\n",
    "        mlp_out = self.MLP(second_norm)\n",
    "        output = mlp_out + first_added\n",
    "\n",
    "        return output\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,img_size, patch_size=9, in_channels=3, embedding_dim=512, depth=6, n_heads=6, mlp_ratio=4, p_dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embedding_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embedding.n_patches, embedding_dim))\n",
    "\n",
    "        \n",
    "        self.encoder_blocks = nn.ModuleList([ EncoderBlock(embedding_dim, n_heads, mlp_ratio, p_dropout) for _ in range(depth)])\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : [n_samples, in_channels, img_size, img_size]\n",
    "        output : [n_samples, 1, embedding_dim]\n",
    "        \"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1) #[n_samples, 1, embedding_dim]\n",
    "        x = torch.cat((cls_token, x), dim=1) #[n_samples, 1 + n_pathces, embedding_dim]\n",
    "\n",
    "        x = x + self.pos_embed #[n_samples, 1 + n_pathces, embedding_dim]\n",
    "\n",
    "        for enc_block in self.encoder_blocks:\n",
    "            x = enc_block(x)\n",
    "\n",
    "        return x[:, 0] #[n_samples, 1, embedding_dim] Only extract the token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
