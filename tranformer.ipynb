{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available() # For macOS\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Patch the image (needs to be square) and performs a linear projection of the patchs see : \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels=3, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.n_patches = (self.img_size // patch_size) ** 2\n",
    "        self.proj_layer = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x : [n_batches, in_channels, img_size, img_size]\n",
    "            output : [n_batches, embedding_dim, n_batches]\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.proj_layer(x) #[n_bathces, embedding_dim, sqrt(n_patches), sqrt(n_pathces)]\n",
    "        x = x.flatten(2) #[n_batches, embedding_dim, n_patches]\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4, p_dropout=0.5):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.p_dropout = p_dropout\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.norm = nn.LayerNorm(self.latent_size)\n",
    "        self.attention = nn.MultiheadAttention(self.dim, self.n_heads, dropout=self.p_dropout)\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.dim * mlp_ratio, self.dim),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : [n_samples, n_patches + 1, embedding_dim]\n",
    "        output : [n_samples, n_patches + 1, embedding_dim]\n",
    "        \"\"\"\n",
    "        first_norm = self.norm(x)\n",
    "        attention_out = self.attention(first_norm, first_norm, first_norm)\n",
    "        first_added = attention_out + x\n",
    "        second_norm = self.norm(first_added)\n",
    "        mlp_out = self.MLP(second_norm)\n",
    "        output = mlp_out + first_added\n",
    "\n",
    "        return output\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,img_size, patch_size=9, in_channels=3, embedding_dim=512, depth=6, n_heads=6, mlp_ratio=4, p_dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embedding_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embedding.n_patches, embedding_dim))\n",
    "\n",
    "        \n",
    "        self.encoder_blocks = nn.ModuleList([ EncoderBlock(embedding_dim, n_heads, mlp_ratio, p_dropout) for _ in range(depth)])\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : [n_samples, in_channels, img_size, img_size]\n",
    "        output : [n_samples, 1, embedding_dim]\n",
    "        \"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1) #[n_samples, 1, embedding_dim]\n",
    "        x = torch.cat((cls_token, x), dim=1) #[n_samples, 1 + n_pathces, embedding_dim]\n",
    "\n",
    "        x = x + self.pos_embed #[n_samples, 1 + n_pathces, embedding_dim]\n",
    "\n",
    "        for enc_block in self.encoder_blocks:\n",
    "            x = enc_block(x)\n",
    "\n",
    "        return x #[n_samples, 1 + n_patches, embedding_dim] Only extract the token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/aabibi/IA/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/aabibi/IA/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aabibi/IA/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aabibi/IA/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aabibi/IA/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aabibi/IA/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aabibi/IA/lib/python3.11/site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aabibi/IA/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.5.ln_cross_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.q_attn.bias', 'h.9.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.10.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.11.ln_cross_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.3.crossattention.c_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.bias', 'h.11.ln_cross_attn.bias', 'h.5.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.weight', 'h.4.ln_cross_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.3.ln_cross_attn.bias', 'h.9.ln_cross_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.2.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.11.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.8.ln_cross_attn.bias', 'h.8.crossattention.c_proj.bias', 'h.2.ln_cross_attn.weight', 'h.6.ln_cross_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.4.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.3.ln_cross_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.6.ln_cross_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.bias', 'h.5.ln_cross_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.bias', 'h.7.ln_cross_attn.bias', 'h.1.crossattention.c_proj.bias', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.2.ln_cross_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.7.crossattention.c_attn.bias', 'h.1.ln_cross_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.10.ln_cross_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_attn.bias', 'h.7.ln_cross_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.1.ln_cross_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.9.ln_cross_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.weight', 'h.5.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-12-06 16:49:02.590488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-06 16:49:03.335095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/aabibi/IA/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
    "\n",
    "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "text_decoder_model = \"gpt2\"\n",
    "\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(image_encoder_model, text_decoder_model)\n",
    "\n",
    "\n",
    "#Get feature extractor from yhe image encoder model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)\n",
    "\n",
    "#Get tokenizer from the text decoder model\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_decoder_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'decoder.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('model/tokenizer_config.json',\n",
       " 'model/special_tokens_map.json',\n",
       " 'model/vocab.json',\n",
       " 'model/merges.txt',\n",
       " 'model/added_tokens.json',\n",
       " 'model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = \"model\"\n",
    "model.save_pretrained(out_dir)\n",
    "feature_extractor.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Images'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions from the text file\n",
    "with open(os.path.join('./', 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e115d1ca0fc04eedae3fddc995964f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create mapping of image to captions\n",
    "mapping = {}\n",
    "img_ids = []\n",
    "\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    image_id = image_id.split('.')[0]\n",
    "    caption = \" \".join(caption)\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    mapping[image_id].append(caption)\n",
    "    img_ids.append(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing step\n",
    "def tokenization_caption(img_id, max_caption_length):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    labels = tokenizer(mapping[img_id],\n",
    "                      truncation=True,\n",
    "                      padding=\"max_length\", \n",
    "                      max_length=max_caption_length).input_ids\n",
    "\n",
    "    return labels\n",
    "\n",
    "def feature_extraction(img_id):\n",
    "    image = Image.open(img_id + 'jpg')\n",
    "\n",
    "    encoder_input = feature_extractor(images=transform(image), return_tensor=\"np\")\n",
    "\n",
    "    return encoder_input.pixel_values\n",
    "\n",
    "\n",
    "def get_model_input(img_id, max_caption_length):\n",
    "\n",
    "    model_input = {}\n",
    "\n",
    "    model_input['labels'] = tokenization_caption(img_id, max_caption_length)\n",
    "\n",
    "    model_input['pixel_values'] = feature_extraction(img_id)\n",
    "\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, data_keys, max_caption):\n",
    "        self.data_keys = data_keys\n",
    "        self.max_caption = max_caption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_keys)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        key = self.data_keys[index]\n",
    "\n",
    "        features = feature_extraction(key)\n",
    "        labels = tokenization_caption(key, self.max_caption)\n",
    "\n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 35\n",
    "split = int(len(img_ids) * 0.75)\n",
    "train_ids = img_ids[:split]\n",
    "test_ids = img_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = CaptionDataset(train_ids, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = CaptionDataset(test_ids, max_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./image-captioning-output\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
